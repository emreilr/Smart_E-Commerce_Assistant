from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# --- AYARLAR ---
BASE_MODEL = "Trendyol/Trendyol-LLM-7b-base-v1.0"
ADAPTER_PATH = "models/eticaret-uzmani-llm"

app = FastAPI()

# CORS AyarlarÄ± (React uygulamasÄ±nÄ±n eriÅŸebilmesi iÃ§in)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # GÃ¼venlik iÃ§in production'da spesifik domain girilmeli
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global deÄŸiÅŸkenler
model = None
tokenizer = None

class YorumGiris(BaseModel):
    yorum: str

@app.on_event("startup")
async def startup_event():
    global model, tokenizer
    print("ðŸš€ Model yÃ¼kleniyor... LÃ¼tfen bekleyin.")
    
    try:
        # Tokenizer YÃ¼kle
        tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)
        
        # 4-bit Quantization AyarlarÄ± (GPU Bellek Tasarrufu iÃ§in)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )

        # Base Modeli YÃ¼kle
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            device_map="auto"
        )
        
        # EÄŸitilmiÅŸ LoRA Adapter'Ä± Base Model Ã¼zerine ekle
        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
        model.eval()
        
        print("Model baÅŸarÄ±yla yÃ¼klendi ve analize hazÄ±r!")
    except Exception as e:
        print(f" Model yÃ¼klenirken hata oluÅŸtu: {e}")

@app.post("/analiz-et")
async def analiz_et(veri: YorumGiris):
    if not model:
        raise HTTPException(status_code=503, detail="Model henÃ¼z yÃ¼klenmedi.")

    yorum = veri.yorum
    
    # Prompt HazÄ±rlÄ±ÄŸÄ± (Modelin eÄŸitim formatÄ±na uygun olmalÄ±)
    # Basit bir "instruction" formatÄ± deniyoruz.
    prompt = f"### Talimat: AÅŸaÄŸÄ±daki yorumu e-ticaret baÄŸlamÄ±nda analiz et. Niyet ve Duygu durumunu JSON formatÄ±nda dÃ¶ndÃ¼r.\n### Yorum: {yorum}\n### Cevap: "
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=100, 
            do_sample=True, 
            temperature=0.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Sadece Ã¼retilen yeni tokenlarÄ± al
    input_len = inputs.input_ids.shape[1]
    generated_ids = outputs[0][input_len:]
    analiz_sonucu = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # HalÃ¼sinasyonlarÄ± (tekrar eden ### Yorum vs.) temizle
    analiz_sonucu = analiz_sonucu.split("###")[0].strip()
    
    # JSON Ã‡Ä±karÄ±mÄ±
    import json
    import re
    
    duygu = "NÃ¶tr"
    niyet = "Genel"
    
    try:
        # JSON benzeri yapÄ±yÄ± bulmaya Ã§alÄ±ÅŸ
        json_match = re.search(r"\{.*?\}", analiz_sonucu)
        if json_match:
            data = json.loads(json_match.group(0))
            raw_duygu = data.get("Duygu", "NÃ¶tr")
            niyet = data.get("Niyet", "Genel")
            
            # Frontend uyumluluÄŸu iÃ§in haritalama
            if raw_duygu == "Olumlu":
                duygu = "Pozitif"
            elif raw_duygu == "Olumsuz":
                duygu = "Negatif"
            else:
                duygu = raw_duygu
        else:
            print(f"UYARI: JSON deseni bulunamadÄ±. Ham Ã§Ä±ktÄ±: {analiz_sonucu}")
    except Exception as e:
        print(f"JSON Parse HatasÄ±: {e}. Ham Ã‡Ä±ktÄ±: {analiz_sonucu}")

    return {
        "duygu": duygu,
        "niyet": niyet,
        "analiz_sonucu": analiz_sonucu
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
